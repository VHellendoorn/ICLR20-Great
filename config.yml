model:
  configuration: "great"
  base:
    hidden_dim: 512
    dropout_rate: 0.1
    num_edge_types: 24
  rnn:
    num_layers: 2
  ggnn:
    time_steps: [3, 1, 3, 1]
    residuals:  # Note: keys must be strings for TF checkpointing
      "1": [0]
      "3": [0, 1]
    add_type_bias: true
  transformer:
    ff_dim: 2048
    num_layers: 6
    attention_dim: 512
    num_heads: 8
data:
  max_batch_size: 12500
  max_buffer_size: 50  # In terms of no. of (maximum efficiency) batches.
  max_sequence_length: 512
  valid_interval: 250000
  max_valid_samples: 25000
  max_token_length: 10  # In terms of (BPE) sub-tokens.
training:
  max_steps: 100
  print_freq: 25
  learning_rate: 0.0001
